template_name: PreferenceRankingRCT
template_label: PreferenceRankingRCT
template_description: Ranking RCTs
prepend_instructions: false
instructions: |
  ## Instructions

  You will be given three outputs generated from a model inferring the conclusion about randomized control trials.
  Each will be labeled "Output A", "Output B", and "Output C". We want to evaluate which model-generated output is the best.
  The criteria to rank these outputs upon are as follows:

  1. Agreement with the reference conclusion (whether there is enough evidence, not enough evidence, etc.)
  2. Whether the output is fluent and sensible
  3. Whether there is irrelevant information that detracts from the conclusion  
   
  Rank the three targets by selecting their rank accordingly. Ranking is ordered 1 for the best output and 3 for the worst output.
  However, if some outputs are about the same, feel free to rank them the same. For instance: A = 1, B = 2, and C = 2.

  #### Workflow
  Click "Add Edit" > Hit "Output A", "Output B", or "Output C". > Click "Save".

  After, click the edit button (pencil) to edit > Answer the question by selecting a ranking > Hit "Save" to save your selection.

  Do the above for all three edits: "Output A", "Output B", and "Output C".

display:
- side-by-side

interface_text:
  typology:
    source_label: "Reference Summary"
    target_label: "Generated Summaries"

# "Edits" are our units of annotations. First, edits are selected, then annotated.
edits:
  - name: output_ranking_a
    label: "Output A"
    enable_output: false
    color: red
    icon: fa-plus
    annotation:
      - name: preference_ranking_a
        label: "Ranking A"
        question: "What is the ranking of Output A?"
        options: 
          - name: "1"
            label: "1"
          - name: "2"
            label: "2"
          - name: "3"
            label: "3"

  - name: output_ranking_b
    label: "Output B"
    enable_output: false
    color: blue
    icon: fa-plus
    annotation:
      - name: preference_ranking_b
        label: "Ranking B"
        question: "What is the ranking of Output B?"
        options: 
          - name: "1"
            label: "1"
          - name: "2"
            label: "2"
          - name: "3"
            label: "3"

  - name: output_ranking_c
    label: "Output C"
    enable_output: false
    color: green
    icon: fa-plus
    annotation:
      - name: preference_ranking_c
        label: "Ranking C"
        question: "What is the ranking of Output C?"
        options: 
          - name: "1"
            label: "1"
          - name: "2"
            label: "2"
          - name: "3"
            label: "3" 

disable:
  - download
  - upload

crowdsource: "prolific"       # Use "prolific" or other crowdsource options here, "custom" for your own database setup
prolific_completion_code: C8QPCDND
completion_code: C8QPCDND
database:
    type: firebase
    project_id: granularity-preference-r-3c66a
    url: https://granularity-preference-r-3c66a.firebaseio.com/
    collection: thresh     # (default: thresh) The database to use, this must exist on the firestore
    document: annotations   # (default: annotation) The document to use, this must exist on the firestore
    field: annotation_set_1  # The document field to store annotations, we recommend changing this value between data collection runs